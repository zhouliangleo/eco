------------------------------------
Environment Versions:
- Python: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) 
[GCC 7.3.0]
- PyTorch: 0.4.0
- TorchVison: 0.2.1
------------------------------------
ECO Configurations:
- dataset: actev
- modality: RGB
- train_list: /home/leo/data/nactev/list/2018actev_train_file.txt
- val_list: /home/leo/data/nactev/list/2018actev_val_file.txt
- arch: ECO
- num_segments: 30
- consensus_type: identity
- pretrained_parts: finetune
- k: 3
- dropout: 0.3
- loss_type: nll
- epochs: 40
- batch_size: 5
- iter_size: 4
- lr: 0.001
- lr_steps: [15.0, 30.0]
- momentum: 0.9
- weight_decay: 0.0005
- clip_gradient: 50.0
- no_partialbn: True
- nesterov: True
- print_freq: 20
- eval_freq: 1
- workers: 5
- resume: net_runs/ECO_30_finetune
- evaluate: False
- snapshot_pref: net_runs/ECO_30_finetune/eco_lite
- start_epoch: 0
- gpus: None
- flow_prefix: 
- rgb_prefix: 
------------------------------------

Initializing TSN with base model: ECO.
TSN Configurations:
    input_modality:     RGB
    num_segments:       30
    new_length:         1
    consensus_module:   identity
    dropout_ratio:      0.3
        
pretrained_parts:  finetune
=> loading model 'models/eco_lite_rgb_16F_kinetics_v2.pth.tar'
**************************************************
Start finetuning ..
un_init_dict_keys:  ['module.base_model.res3a_2.bias', 'module.base_model.res3b_1.bias', 'module.base_model.res3b_2.bias', 'module.base_model.res4a_1.bias', 'module.base_model.res4a_2.bias', 'module.base_model.res4a_down.bias', 'module.base_model.res4b_1.bias', 'module.base_model.res4b_2.bias', 'module.base_model.res5a_1.bias', 'module.base_model.res5a_2.bias', 'module.base_model.res5a_down.bias', 'module.base_model.res5b_1.bias', 'module.base_model.res5b_2.bias', 'module.new_fc.weight', 'module.new_fc.bias']

------------------------------------
module.base_model.res3a_2.bias init as: 0
module.base_model.res3b_1.bias init as: 0
module.base_model.res3b_2.bias init as: 0
module.base_model.res4a_1.bias init as: 0
module.base_model.res4a_2.bias init as: 0
module.base_model.res4a_down.bias init as: 0
module.base_model.res4b_1.bias init as: 0
module.base_model.res4b_2.bias init as: 0
module.base_model.res5a_1.bias init as: 0
module.base_model.res5a_2.bias init as: 0
module.base_model.res5a_down.bias init as: 0
module.base_model.res5b_1.bias init as: 0
module.base_model.res5b_2.bias init as: 0
module.new_fc.weight init as: xavier
module.new_fc.bias init as: 0
------------------------------------
=> no checkpoint found at 'net_runs/ECO_30_finetune'
group: first_3d_conv_weight has 1 params, lr_mult: 1, decay_mult: 1
group: first_3d_conv_bias has 1 params, lr_mult: 2, decay_mult: 0
group: normal_weight has 32 params, lr_mult: 1, decay_mult: 1
group: normal_bias has 32 params, lr_mult: 2, decay_mult: 0
group: BN scale/shift has 60 params, lr_mult: 1, decay_mult: 0
No BN layer Freezing.
Traceback (most recent call last):
  File "main.py", line 505, in <module>
    main()
  File "main.py", line 191, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "main.py", line 347, in train
    output = model(input_var)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 114, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 124, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 65, in parallel_apply
    raise output
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 41, in _worker
    output = module(*input, **kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/leo/ECO-pytorch/models.py", line 352, in forward
    base_out = self.new_fc(base_out)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 55, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py", line 992, in linear
    return torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [3 x 1024], m2: [512 x 4] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249
------------------------------------
Environment Versions:
- Python: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) 
[GCC 7.3.0]
- PyTorch: 0.4.0
- TorchVison: 0.2.1
------------------------------------
ECO Configurations:
- dataset: actev
- modality: RGB
- train_list: /home/leo/data/nactev/list/2018actev_train_file.txt
- val_list: /home/leo/data/nactev/list/2018actev_val_file.txt
- arch: ECO
- num_segments: 30
- consensus_type: identity
- pretrained_parts: finetune
- k: 3
- dropout: 0.3
- loss_type: nll
- epochs: 40
- batch_size: 5
- iter_size: 4
- lr: 0.001
- lr_steps: [15.0, 30.0]
- momentum: 0.9
- weight_decay: 0.0005
- clip_gradient: 50.0
- no_partialbn: True
- nesterov: True
- print_freq: 20
- eval_freq: 1
- workers: 5
- resume: net_runs/ECO_30_finetune
- evaluate: False
- snapshot_pref: net_runs/ECO_30_finetune/eco_lite
- start_epoch: 0
- gpus: None
- flow_prefix: 
- rgb_prefix: 
------------------------------------

Initializing TSN with base model: ECO.
TSN Configurations:
    input_modality:     RGB
    num_segments:       30
    new_length:         1
    consensus_module:   identity
    dropout_ratio:      0.3
        
pretrained_parts:  finetune
=> loading model 'models/eco_lite_rgb_16F_kinetics_v2.pth.tar'
**************************************************
Start finetuning ..
un_init_dict_keys:  ['module.base_model.res3a_2.bias', 'module.base_model.res3b_1.bias', 'module.base_model.res3b_2.bias', 'module.base_model.res4a_1.bias', 'module.base_model.res4a_2.bias', 'module.base_model.res4a_down.bias', 'module.base_model.res4b_1.bias', 'module.base_model.res4b_2.bias', 'module.base_model.res5a_1.bias', 'module.base_model.res5a_2.bias', 'module.base_model.res5a_down.bias', 'module.base_model.res5b_1.bias', 'module.base_model.res5b_2.bias', 'module.new_fc.weight', 'module.new_fc.bias']

------------------------------------
module.base_model.res3a_2.bias init as: 0
module.base_model.res3b_1.bias init as: 0
module.base_model.res3b_2.bias init as: 0
module.base_model.res4a_1.bias init as: 0
module.base_model.res4a_2.bias init as: 0
module.base_model.res4a_down.bias init as: 0
module.base_model.res4b_1.bias init as: 0
module.base_model.res4b_2.bias init as: 0
module.base_model.res5a_1.bias init as: 0
module.base_model.res5a_2.bias init as: 0
module.base_model.res5a_down.bias init as: 0
module.base_model.res5b_1.bias init as: 0
module.base_model.res5b_2.bias init as: 0
module.new_fc.weight init as: xavier
module.new_fc.bias init as: 0
------------------------------------
=> no checkpoint found at 'net_runs/ECO_30_finetune'
group: first_3d_conv_weight has 1 params, lr_mult: 1, decay_mult: 1
group: first_3d_conv_bias has 1 params, lr_mult: 2, decay_mult: 0
group: normal_weight has 32 params, lr_mult: 1, decay_mult: 1
group: normal_bias has 32 params, lr_mult: 2, decay_mult: 0
group: BN scale/shift has 60 params, lr_mult: 1, decay_mult: 0
No BN layer Freezing.
Traceback (most recent call last):
  File "main.py", line 505, in <module>
    main()
  File "main.py", line 191, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "main.py", line 347, in train
    output = model(input_var)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 114, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 124, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 65, in parallel_apply
    raise output
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 41, in _worker
    output = module(*input, **kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/leo/ECO-pytorch/models.py", line 352, in forward
    base_out = self.new_fc(base_out)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 55, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/leo/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py", line 992, in linear
    return torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [3 x 1024], m2: [512 x 4] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249
